{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Blueprint\n",
    "\n",
    "The basics of a neural network is to: **Build** -> **Train** -> **Test**\n",
    "\n",
    "This will be demonstrated below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nerual Network(NN)\n",
    "Typically a NN has an **Input** layer, **Hidden** layer/s, and an **Output** layer.\n",
    "At each of these steps there's data pre-processing, some random/non-random weight initialization, an squashing function(to put all numbers between 0-1 or -1-1), and some form of backpropagation with gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:0.5028737600123812\n",
      "Error:0.00911646534335037\n",
      "Error:0.006295398146737811\n",
      "Error:0.005085035346132255\n",
      "Error:0.0043746563305548155\n",
      "Output after training:\n",
      "[[0.00215897]\n",
      " [0.99640868]\n",
      " [0.99552582]\n",
      " [0.00535417]]\n"
     ]
    }
   ],
   "source": [
    "#Setup\n",
    "#use numpy since it's a good library for scientific computing\n",
    "import numpy as np\n",
    "\n",
    "#create a function to map any value to a value between 0-1(we use the sigmoid function here)\n",
    "def sigmoid(x,deriv=False):\n",
    "    if(deriv==True):\n",
    "        return x*(1-x)\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#initialize input data set as a matrix\n",
    "#(each row is a different training example, each column is a different neuron)\n",
    "X = np.array([[0,0,1],[0,1,1],[1,0,0],[1,1,1]])\n",
    "\n",
    "#output data set\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "#here we'll seed our random number so that it's random but generates \n",
    "#the same number each time(useful for debugging)\n",
    "np.random.seed(1)\n",
    "\n",
    "#create connections between matrixes(neuron in one leyer to neuron in next layer)\n",
    "#since this NN is 3 layers, we need 2 synapse matrixes with random weights\n",
    "syn0 = 2*np.random.random((3,4)) - 1\n",
    "syn1 = 2*np.random.random((4,1)) - 1\n",
    "\n",
    "#training step\n",
    "for j in range(50000):\n",
    "    #create first layer(just the input data)\n",
    "    l0 = X\n",
    "    #create prediction between each layer and its synapse(dot product)\n",
    "    #then create the next layer(sigmoid function over all values in matrix)\n",
    "    l1 = sigmoid(np.dot(l0, syn0))\n",
    "    #this next layer contains prediction of output data\n",
    "    #do same steps as previous layer to get more refined prediction(for output value)\n",
    "    l2 = sigmoid(np.dot(l1, syn1))\n",
    "    \n",
    "    #compare predicted output value(l2 prediction above) to expected output value\n",
    "    l2_error = y - l2\n",
    "    \n",
    "    #print out average error rate at set intervals so we can make sure it goes down each training iteration\n",
    "    if(j % 10000) == 0:\n",
    "        print (\"Error:\" + str(np.mean(np.abs(l2_error))))\n",
    "        \n",
    "    #multiply erorr rate by sigmoid to get derivative of output prediction from l2\n",
    "    #this will give us a delta which we will use to update synapses every\n",
    "    #iteration of training to reduce error rate\n",
    "    l2_delta = l2_error*sigmoid(l2, deriv=True)\n",
    "    \n",
    "    #check how much l1 contributed to the error in l2(a.k.a Backpropagation)\n",
    "    #get this by multiplying l2 delta by synapse 1's transpose\n",
    "    l1_error = l2_delta.dot(syn1.T)\n",
    "    #get l1 delta by multiplying l1 error by result of sigomoid(used to get derivative of l1) \n",
    "    l1_delta = l1_error * sigmoid(l1,deriv=True)\n",
    "    \n",
    "    #now update the synapse weights with each delta layer to continually \n",
    "    #reduce error rate every iteration(a.k.a Gradient Descent)\n",
    "    syn1 += l1.T.dot(l2_delta)\n",
    "    syn0 += l0.T.dot(l1_delta)\n",
    "    \n",
    "#print the output\n",
    "print (\"Output after training:\")\n",
    "print (l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We can see the error rate decreasing for each iteration and the predicted output is really close to the actual output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
